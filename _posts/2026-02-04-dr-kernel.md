---
layout: paper_blog
title: "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generation"
subtitle: "A practical roadmap for RL-based Triton kernel generation: robust execution infrastructure, unbiased multi-turn credit assignment, and bottleneck-aware optimization."
authors: "Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He"
date: 2026-02-04
categories: [kernel, triton, rl]
paper_url: ""
code_url: "https://github.com/hkust-nlp/KernelGYM"
project_url: ""
---

Training LLMs to write fast Triton kernels is appealing: correctness is executable, and speedup is measurable.
But in practice, RL for this task fails in subtle ways unless the environment, objective, and credit assignment are all designed carefully.

This post explains what worked for us in **Dr. Kernel**: the system design, the RL method, and the training objective changes that turned unstable progress into meaningful speedup.

<figure class="fun-poster">
  <img src="{{ '/assets/img/dr-kernel/twitter.png' | relative_url }}" alt="Dr. Kernel teaser poster" />
  <figcaption>
    A playful Dr. Kernel visual used in our launch thread.
  </figcaption>
</figure>

## TL;DR

Our pipeline has three parts:

1. **KernelGym**: a distributed, fault-tolerant GPU environment with execution-based hacking checks and profiling feedback.
2. **Multi-turn RL with TRLOO**: unbiased turn-level advantage estimation for iterative kernel refinement.
3. **MRS + PR/PRS**: stabilize training under rollout/training mismatch, then align rewards to real bottlenecks.

On KernelBench Level-2 (Fast@1.2):
- **Dr. Kernel-14B**: **25.6**
- **Claude-4.5-Sonnet**: **26.7**
- **GPT-5**: **28.6**
- **Dr. Kernel-14B + STTS (last turn)**: **31.6**
- **Dr. Kernel-14B + STTS (best-of-history)**: **47.8**

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1-in-twitter.png' | relative_url }}" alt="Dr. Kernel level-2 results compared to frontier models" />
  <figcaption>
    Level-2 Fast@1.2 highlight: with STTS, Dr. Kernel exceeds GPT-5 and Claude-4.5-Sonnet on this subset.
  </figcaption>
</figure>

## The Two Failure Modes We Kept Hitting

Kernel generation looks clean from an RL perspective, but two pathologies dominate training dynamics:

- **Reward hacking**: the model finds ways to “look correct/fast” without running meaningful Triton compute.
- **Lazy optimization**: the model optimizes easy fragments while real runtime bottlenecks remain untouched.

<figure>
  <img src="{{ '/assets/img/dr-kernel/tradeoff_all_three.png' | relative_url }}" alt="Training curves and pitfalls cases (reward hacking and lazy optimization)" />
  <figcaption>
    Fast@1 can continue improving while Fast@1.2 saturates early, a classic signal of low-impact optimization.
  </figcaption>
</figure>

<figure class="case-study-figure">
  <img src="{{ '/assets/img/dr-kernel/casestudy-in-twitter.png' | relative_url }}" alt="Case studies of reward hacking and lazy optimization" />
  <figcaption>
    Case studies: reward hacking and lazy optimization trajectories.
  </figcaption>
</figure>

## KernelGym: Making Long-Horizon Kernel RL Actually Run

Generated kernels frequently trigger compilation/runtime faults.
If the environment is brittle, the RL pipeline fails before the model learns anything useful.

KernelGym is designed around this reality:
- server–worker scheduling for scalable evaluation,
- one-GPU-one-task serialized execution for reliable timing,
- subprocess isolation to contain CUDA/runtime crashes,
- structured tool feedback (correctness, errors, profiling),
- execution-based hacking checks in both `train` and `eval` modes.

One practical rule is especially important: if no Triton kernel is actually launched, the sample is marked incorrect.
This removes a large class of superficially high-reward but invalid trajectories.

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1.png' | relative_url }}" alt="KernelGym overview and training framework" />
  <figcaption>
    Training stack overview: KernelGym infrastructure + multi-turn RL methods.
  </figcaption>
</figure>

## Multi-Turn RL: Better Credit Assignment Matters

Kernel optimization is iterative in nature, so we train with multi-turn refinement: propose → execute → diagnose → revise.
The environment returns dense feedback at every turn, not just a single terminal signal.

In this setup, we found a bias issue with GRPO-style baselines due to self-inclusion at the prompt-turn group level.
We use **TRLOO (Turn-level REINFORCE Leave-One-Out)** to keep turn-level advantage estimation unbiased.
Empirically, this yields more stable and stronger learning curves.

<figure>
  <img src="{{ '/assets/img/dr-kernel/multiturn_rl_comparison.png' | relative_url }}" alt="Multi-turn RL comparison (Fast@1) over training" />
  <figcaption>
    Multi-turn RL variants on KernelBench Level-2 (Fast@1).
  </figcaption>
</figure>

## From Stability to Real Speedup: MRS, PR, and PRS

Even with stable infrastructure and unbiased advantage estimates, Fast@1.2 can plateau early.
The core issue is objective mismatch: many “valid” trajectories carry little performance value.

We handle this in two stages:

1. **MRS** for rollout/training mismatch correction (stability).
2. **PR + PRS** for bottleneck-aware optimization (effectiveness).

- **PR (Profiling-based Rewards)** adds reward for generated kernels that actually cover substantial CUDA runtime.
- **PRS (Profiling-based Rejection Sampling)** reduces the contribution of low-impact samples.

This difference is concrete in case studies:
lazy solutions may contribute only **0.014%** of CUDA time, while better fusion solutions can cover **86.15%**.

<figure>
  <img src="{{ '/assets/img/dr-kernel/method-in-twitter.png' | relative_url }}" alt="MRS and PR/PRS improve both scores and stability" />
  <figcaption>
    MRS plus PR/PRS improve both Fast@1.2 and training stability.
  </figcaption>
</figure>

## Main Results and Sequential Test-Time Scaling

We evaluate under strict execution checks (hacked samples count as incorrect), so the metrics are conservative.

### KernelBench Level-2 (Fast@1.2)

| Model | Fast@1 | Fast@1.2 |
| --- | ---:| ---:|
| GPT-5 | 46.7 | 28.6 |
| Claude-4.5-Sonnet | 50.0 | 26.7 |
| Dr. Kernel-14B | 49.2 | 25.6 |
| Dr. Kernel-14B-STTS (last turn) | 59.8 | 31.6 |
| Dr. Kernel-14B-STTS (best-of-history) | 80.9 | 47.8 |

**STTS (Sequential Test-Time Scaling)** increases refinement turns at inference.
With context management, we keep only top-reward turns in prompt context (`w=4`) while storing full history externally.
This significantly improves Level-2 Fast@1.2.

<figure>
  <img src="{{ '/assets/img/dr-kernel/last_turn.png' | relative_url }}" alt="STTS last-turn Fast@1.2" />
  <figcaption>
    STTS, last-turn metric.
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/best_of_history.png' | relative_url }}" alt="STTS best-of-history Fast@1.2" />
  <figcaption>
    STTS, best-of-history metric.
  </figcaption>
</figure>

## A Stronger Baseline Check: `torch.compile`

We also evaluate under `torch.compile`, which is a stronger baseline than eager mode.
Fast@p values are lower for all methods, but this setting better reflects real incremental gains.
Dr. Kernel remains competitive under this stricter evaluation.

<figure>
  <img src="{{ '/assets/img/dr-kernel/compile.png' | relative_url }}" alt="Results under torch.compile" />
  <figcaption>
    `torch.compile` evaluation: a stronger baseline where Dr. Kernel remains competitive.
  </figcaption>
</figure>

## Limitations and Next Steps

Three directions are likely the highest leverage:

- **Data scale**: expand cold-start and domain-specific kernel data.
- **Model scale**: larger models still show clear gains in this regime.
- **Productionization**: close the gap between strong generated snippets and fully autonomous end-to-end kernel optimization.

## Closing

The practical lesson is simple: for kernel generation RL, better rewards and better policy gradients are not enough by themselves.
You need the full stack—reliable execution infrastructure, unbiased multi-turn learning, and bottleneck-aware optimization signals—to consistently improve meaningful speedup.
