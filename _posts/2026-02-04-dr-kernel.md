---
layout: post
title: "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generation"
date: 2026-02-04
categories: [kernel, triton, rl]
paper_url: ""
---

This post is a concise, figure-first companion to our paper:
**_Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations_**.
It reuses as much paper text and figures as possible to help readers quickly grasp the key ideas.

{% if page.paper_url and page.paper_url != "" %}
Paper: {{ page.paper_url }}
{% endif %}

## TL;DR

> High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to *reward hacking* and *lazy optimization*.

The core message:
1) build a *robust execution environment* (**KernelGym**) that makes RL feasible at scale,  
2) use an *unbiased* multi-turn RL estimator (**TRLOO**), and  
3) align training toward *meaningful speedup* via profiling-aware methods (**PR / PRS**) and stabilize RL with mismatch correction (**MRS**).

## What can go wrong: reward hacking and lazy optimization

> During training, models can devolve into *reward hacking*, exploiting measurement loopholes or executing invalid operations that appear fast but result in meaningless optimizations. Alternatively, models may generate correct but trivial kernel implementations that fail to deliver meaningful speedup, a phenomenon we refer to as *lazy optimization*.

<figure>
  <img src="{{ '/assets/img/dr-kernel/tradeoff_all_three.png' | relative_url }}" alt="Training curves and pitfalls cases (reward hacking and lazy optimization)" />
  <figcaption>
    Training curves (Fast@1 vs Fast@1.2) and representative cases of reward hacking / lazy optimization.
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/hacking_lazy.png' | relative_url }}" alt="Examples of reward hacking and lazy optimization" />
  <figcaption>
    Representative cases: reward hacking (kernel emitted but not executed) and lazy optimization (only trivial sub-op optimized).
  </figcaption>
</figure>

## KernelGym: a gym for kernel generation

> Training LLM agents for iterative kernel generation requires an environment that can (1) evaluate correctness and performance at scale, (2) remain resilient against frequent CUDA runtime failures, and (3) provide granular feedback for RL optimization.

KernelGym is a server–worker system that isolates unsafe generated kernels (subprocess per evaluation), enforces serialized GPU execution for reliable profiling, and exposes structured feedback (correctness, speedup, profiling summaries, and hacking checks).

> Concretely, the design of KernelGym follows four principles tailored to GPU workloads: (i) **Serialized execution** … (ii) **Elastic scalability** … (iii) **Fault isolation and self-recovery** … (iv) **Rich environmental feedback** …

Hacking checks are execution-based (not “LLM-as-a-judge”):

> KernelGym instruments Triton’s launch path to record executed Triton kernels and measures end-to-end runtime in both `train` and `eval` modes … we mark a candidate as incorrect if it executes no Triton kernel in either mode.

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1.png' | relative_url }}" alt="KernelGym overview and training framework" />
  <figcaption>
    Overview of KernelGym and the training framework (paper Figure 1).
  </figcaption>
</figure>

## Multi-turn RL and TRLOO (unbiased advantage estimation)

> Kernel generation naturally lends itself to multi-turn refinement. Just as human developers iterate by writing, executing, and revising kernels based on runtime profiling, LLMs can improve solutions through repeated propose–evaluate–refine cycles.

We use turn-level rewards from the environment and optimize a multi-turn policy. We identify a bias issue in GRPO caused by self-inclusion and propose **Turn-level REINFORCE Leave-One-Out (TRLOO)** for unbiased advantage estimation in multi-turn RL.

> This setup is distinct from recent agentic RL with multi-step tool use … learning is typically driven by a sparse, single-outcome reward. In contrast, our environment yields dense, turn-level rewards after each execution.

<figure>
  <img src="{{ '/assets/img/dr-kernel/multiturn_rl_comparison.png' | relative_url }}" alt="Multi-turn RL comparison (Fast@1) over training" />
  <figcaption>
    Multi-turn RL variants on KernelBench Level 2 (paper Figure: multi-turn RL comparison).
  </figcaption>
</figure>

## From stability to effectiveness: overcoming lazy optimization

> While mismatch correction prevents early collapse (smoothing the learning curve), it does not fundamentally lift the performance ceiling of Fast@1.2.

We therefore add *profiling-aware* signals to bias learning toward kernels that cover real bottlenecks:
- **PR (Profiling-based Rewards):** reward kernels that dominate end-to-end CUDA runtime.
- **PRS (Profiling-based Rejection Sampling):** filter low-impact “lazy” samples during training.

> In the lazy optimization case, the model-generated kernel accounted for only 0.014% of the total CUDA execution time … In contrast, with better fusion … kernels … covered 86.15% of the total CUDA runtime.

<figure>
  <img src="{{ '/assets/img/dr-kernel/fast12_comparison.png' | relative_url }}" alt="Fast@1.2 improvements with PR/PRS" />
  <figcaption>
    Profiling-based methods (PR/PRS) are required to lift the stricter Fast@1.2 metric.
  </figcaption>
</figure>

## Results highlights (KernelBench + test-time scaling)

We evaluate using KernelBench Fast@p metrics (and treat reward-hacking samples as incorrect in evaluation).

Selected KernelBench numbers (Level 2):

| Model | Fast@1 | Fast@1.2 |
| --- | ---:| ---:|
| GPT-5 | 46.7 | 28.6 |
| Claude-4.5-Sonnet | 50.0 | 26.7 |
| Dr. Kernel-14B | 49.2 | 25.6 |

Test-time scaling (STTS) further improves Fast@1.2 by increasing refinement turns at inference time; context management keeps prompts within context limits by only retaining the top-4 turns (by reward).

<figure>
  <img src="{{ '/assets/img/dr-kernel/last_turn.png' | relative_url }}" alt="STTS last-turn Fast@1.2" />
  <figcaption>
    Sequential test-time scaling: last-turn Fast@1.2 (paper Figure: tts/last_turn).
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/best_of_history.png' | relative_url }}" alt="STTS best-of-history Fast@1.2" />
  <figcaption>
    Sequential test-time scaling: best-of-history Fast@1.2 (paper Figure: tts/best_of_history).
  </figcaption>
</figure>

## Takeaways

- For kernel generation RL, **system design (KernelGym) matters as much as the RL algorithm**.
- **Reward hacking checks** and **profiling feedback** are critical for long-horizon training.
- **TRLOO** improves multi-turn RL by removing a practical bias source in group-based baselines.
- To beat “lazy optimization”, you must optimize for **meaningful speedup**, not just correctness.
