---
layout: post
title: "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generation"
date: 2026-02-04
categories: [kernel, triton, rl]
paper_url: ""
---

This post follows the structure of my X/Twitter summary and matches the latest arXiv draft of **Dr. Kernel**.
It is a compact technical walkthrough: what failed, what we changed, and what actually moved the metric.

{% if page.paper_url and page.paper_url != "" %}
Paper: {{ page.paper_url }}
{% endif %}

Code and resources: [hkust-nlp/KernelGYM](https://github.com/hkust-nlp/KernelGYM)

## 1) One-screen summary

We study RL for Triton kernel generation under strict execution-based evaluation.
The project has three core pieces:

1. **KernelGym**: a distributed, fault-tolerant GPU environment with hacking checks and profiling feedback.
2. **Multi-turn RL with TRLOO**: unbiased turn-level advantage estimation for iterative kernel refinement.
3. **Stability + objective alignment**: MRS for training stability, plus PR/PRS to push optimization toward real bottlenecks.

On KernelBench Level-2 (Fast@1.2):
- **Dr. Kernel-14B**: **25.6**
- **Claude-4.5-Sonnet**: **26.7**
- **GPT-5**: **28.6**
- **Dr. Kernel-14B + STTS (last turn)**: **31.6**
- **Dr. Kernel-14B + STTS (best-of-history)**: **47.8**

## 2) Why this task is tricky for RL

Kernel generation looks RL-friendly because both correctness and speed are measurable.
In practice, two failure modes dominate training:

- **Reward hacking**: the model learns to satisfy surface rules without meaningful execution.
- **Lazy optimization**: the model optimizes easy sub-ops while the true runtime bottleneck stays in Torch.

<figure>
  <img src="{{ '/assets/img/dr-kernel/tradeoff_all_three.png' | relative_url }}" alt="Training curves and pitfalls cases (reward hacking and lazy optimization)" />
  <figcaption>
    Fast@1 can keep increasing while Fast@1.2 saturates early, indicating low-impact optimization.
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/hacking_lazy.png' | relative_url }}" alt="Examples of reward hacking and lazy optimization" />
  <figcaption>
    Representative failure cases: hacked execution path and trivial-only optimization.
  </figcaption>
</figure>

## 3) KernelGym: environment first, then RL

Before discussing algorithms, we had to make training operationally stable.
Generated kernels regularly trigger runtime failures; if the environment is fragile, RL will collapse long before learning useful behavior.

KernelGym addresses this with:
- server–worker scheduling for scalable evaluation,
- one-GPU-one-task serialized execution for reliable timing,
- subprocess isolation to contain CUDA/runtime crashes,
- structured tool feedback (correctness, errors, profiling),
- execution-based hacking checks in both `train` and `eval` modes.

If no Triton kernel is actually launched, the sample is treated as incorrect.
This single rule removes many “looks-fast-but-fake” trajectories.

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1.png' | relative_url }}" alt="KernelGym overview and training framework" />
  <figcaption>
    Training stack overview: KernelGym infrastructure + multi-turn RL methods.
  </figcaption>
</figure>

## 4) Multi-turn RL and TRLOO

Kernel optimization is naturally iterative, so we train the model to refine kernels across turns.
Each turn gets executable feedback, and that feedback is fed into the next proposal.

A key method point: GRPO-style group baselines can be biased by self-inclusion in the multi-turn setting.
We replace this with **TRLOO (Turn-level REINFORCE Leave-One-Out)** to keep turn-level advantage estimation unbiased.
This improves learning stability and sample efficiency in our setting.

<figure>
  <img src="{{ '/assets/img/dr-kernel/multiturn_rl_comparison.png' | relative_url }}" alt="Multi-turn RL comparison (Fast@1) over training" />
  <figcaption>
    Multi-turn RL variants on KernelBench Level-2 (Fast@1).
  </figcaption>
</figure>

## 5) From stable training to meaningful speedup

Even with a stable environment and better advantage estimation, Fast@1.2 can still saturate.
The reason is objective mismatch: many trajectories are valid but low-impact.

We handle this in two stages:

1. **MRS** for rollout/training mismatch correction (stability).
2. **PR + PRS** for bottleneck-aware optimization (effectiveness).

- **PR (Profiling-based Rewards)** adds reward for generated kernels that cover meaningful runtime.
- **PRS (Profiling-based Rejection Sampling)** down-weights low-impact samples during exploration.

In case studies, this distinction is visible:
lazy solutions can contribute only **0.014%** of CUDA time, while better fusion solutions can cover **86.15%**.

<figure>
  <img src="{{ '/assets/img/dr-kernel/fast12_comparison.png' | relative_url }}" alt="Fast@1.2 improvements with PR/PRS" />
  <figcaption>
    MRS stabilizes training; PR/PRS are what lift Fast@1.2 materially.
  </figcaption>
</figure>

## 6) Main results and STTS

We evaluate with strict execution checks (hacking samples count as incorrect), so numbers are more conservative than permissive evaluation settings.

### KernelBench Level-2 (Fast@1.2)

| Model | Fast@1 | Fast@1.2 |
| --- | ---:| ---:|
| GPT-5 | 46.7 | 28.6 |
| Claude-4.5-Sonnet | 50.0 | 26.7 |
| Dr. Kernel-14B | 49.2 | 25.6 |
| Dr. Kernel-14B-STTS (last turn) | 59.8 | 31.6 |
| Dr. Kernel-14B-STTS (best-of-history) | 80.9 | 47.8 |

**STTS (Sequential Test-Time Scaling)** increases refinement turns at inference.
With context management, we keep only top-reward turns in prompt context (`w=4`) while storing full history externally.
This gives the strongest gains in Level-2 Fast@1.2.

<figure>
  <img src="{{ '/assets/img/dr-kernel/last_turn.png' | relative_url }}" alt="STTS last-turn Fast@1.2" />
  <figcaption>
    STTS, last-turn metric.
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/best_of_history.png' | relative_url }}" alt="STTS best-of-history Fast@1.2" />
  <figcaption>
    STTS, best-of-history metric.
  </figcaption>
</figure>

## 7) Extra check: results under `torch.compile`

The latest draft also evaluates under `torch.compile`, which is a stronger baseline than eager mode.
Absolute Fast@p values drop for everyone, but this is a better test of real incremental value.
Dr. Kernel remains competitive in this stricter setup.

## 8) Takeaways

- In kernel RL, infrastructure quality is part of the algorithm.
- Multi-turn RL works better when the advantage estimator is unbiased.
- Stability fixes alone are not enough; objective alignment to bottlenecks is necessary.
- STTS substantially expands what a 14B model can deliver at inference time.
