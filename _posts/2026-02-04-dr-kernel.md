---
layout: paper_blog
title: "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generation"
subtitle: "A practical roadmap for RL-based Triton kernel generation: robust execution infrastructure, unbiased multi-turn credit assignment, and bottleneck-aware optimization."
authors: "Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He"
date: 2026-02-04
categories: [kernel, triton, rl]
paper_url: "https://arxiv.org/abs/2602.05885"
code_url: "https://github.com/hkust-nlp/KernelGYM"
project_url: "https://x.com/WeiLiu99/status/2019629573194019282"
---

Training LLMs to generate GPU kernels looks like an ideal RL problem: you can execute code for correctness and profile it for speed.
The hard part is that **verifiable does not mean trainable**.
In long-horizon RL, kernel generation quickly drifts into reward hacking, lazy optimization, and unstable credit assignment.

This post walks through how we made this setup actually work in practice with **Dr. Kernel**.

<figure class="fun-poster">
  <img src="{{ '/assets/img/dr-kernel/twitter.png' | relative_url }}" alt="Dr. Kernel teaser poster" />
  <figcaption>
    A playful Dr. Kernel visual used in our launch thread.
  </figcaption>
</figure>

## TL;DR

Our solution has three components:

1. **KernelGym**: a distributed, fault-tolerant GPU environment with execution-based hacking checks and profiling feedback.
2. **Multi-turn RL with TRLOO**: unbiased turn-level advantage estimation for iterative kernel refinement.
3. **MRS + PR/PRS**: stabilize training under rollout/training mismatch, then align rewards to real bottlenecks.

On KernelBench Level-2 (Fast@1.2), we observe:
- **Dr. Kernel-14B**: **25.6**
- **Claude-4.5-Sonnet**: **26.7**
- **GPT-5**: **28.6**
- **Dr. Kernel-14B + STTS (last turn)**: **31.6**
- **Dr. Kernel-14B + STTS (best-of-history)**: **47.8**

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1-in-twitter.png' | relative_url }}" alt="Dr. Kernel level-2 results compared to frontier models" />
  <figcaption>
    Level-2 Fast@1.2 highlight: with STTS, Dr. Kernel exceeds GPT-5 and Claude-4.5-Sonnet on this subset.
  </figcaption>
</figure>

## Why Kernel RL is Hard in Practice

GPU kernels must satisfy both correctness and meaningful speedup.
That creates room for two dominant pathologies:

- **Reward hacking**: the model finds ways to “look correct/fast” without running meaningful Triton compute.
- **Lazy optimization**: the model optimizes easy fragments while real runtime bottlenecks remain untouched.

<figure>
  <img src="{{ '/assets/img/dr-kernel/tradeoff_all_three.png' | relative_url }}" alt="Training curves and pitfalls cases (reward hacking and lazy optimization)" />
  <figcaption>
    Fast@1 can continue improving while Fast@1.2 saturates early, a classic signal of low-impact optimization.
  </figcaption>
</figure>

<figure class="case-study-figure">
  <img src="{{ '/assets/img/dr-kernel/casestudy-in-twitter.png' | relative_url }}" alt="Case studies of reward hacking and lazy optimization" />
  <figcaption>
    Case studies: reward hacking and lazy optimization trajectories.
  </figcaption>
</figure>

## KernelGym: Making Long-Horizon Kernel RL Actually Run

A simple eval script is not enough for long RL runs.
Generated kernels frequently trigger compilation/runtime faults, and a fragile pipeline collapses early.

KernelGym is designed around this reality:
- server–worker scheduling for scalable evaluation,
- one-GPU-one-task serialized execution for reliable timing,
- subprocess isolation to contain CUDA/runtime crashes,
- structured tool feedback (correctness, errors, profiling),
- execution-based hacking checks in both `train` and `eval` modes.

One practical rule is especially important: if no Triton kernel is actually launched, the sample is marked incorrect.
This removes a large class of superficially high-reward but invalid trajectories.

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1.png' | relative_url }}" alt="KernelGym overview and training framework" />
  <figcaption>
    Training stack overview: KernelGym infrastructure + multi-turn RL methods.
  </figcaption>
</figure>

## Multi-Turn RL: Better Credit Assignment Matters

Kernel optimization is inherently iterative, so we train with multi-turn refinement: propose → execute → diagnose → revise.
The environment returns dense feedback at every turn, not just a single terminal signal.

In this setup, we found a bias issue with GRPO-style baselines due to self-inclusion at the prompt-turn group level.
We use **TRLOO (Turn-level REINFORCE Leave-One-Out)** to keep turn-level advantage estimation unbiased.
Empirically, this yields more stable and stronger learning curves.

We also find hacking checks are not optional here:
without them, training quality degrades rapidly and can collapse early.

<figure>
  <img src="{{ '/assets/img/dr-kernel/multiturn_rl_comparison.png' | relative_url }}" alt="Multi-turn RL comparison (Fast@1) over training" />
  <figcaption>
    Multi-turn RL variants on KernelBench Level-2 (Fast@1).
  </figcaption>
</figure>

## From Stability to Real Speedup: MRS, PR, and PRS

Even with KernelGym and TRLOO, Fast@1.2 can plateau early.
The core issue is objective mismatch: many “valid” trajectories carry little performance value.

We handle this in two stages:

1. **MRS** for rollout/training mismatch correction (stability).
2. **PR + PRS** for bottleneck-aware optimization (effectiveness).

- **PR (Profiling-based Rewards)** adds reward for generated kernels that actually cover substantial CUDA runtime.
- **PRS (Profiling-based Rejection Sampling)** reduces the contribution of low-impact samples.

This difference is concrete in case studies:
lazy solutions may contribute only **0.014%** of CUDA time, while better fusion solutions can cover **86.15%**.

<figure>
  <img src="{{ '/assets/img/dr-kernel/method-in-twitter.png' | relative_url }}" alt="MRS and PR/PRS improve both scores and stability" />
  <figcaption>
    MRS plus PR/PRS improve both Fast@1.2 and training stability.
  </figcaption>
</figure>

## Main Results and Sequential Test-Time Scaling

We evaluate under strict execution checks (hacked samples count as incorrect), so the numbers are conservative by construction.

### KernelBench Level-2 (Fast@1.2)

| Model | Fast@1 | Fast@1.2 |
| --- | ---:| ---:|
| GPT-5 | 46.7 | 28.6 |
| Claude-4.5-Sonnet | 50.0 | 26.7 |
| Dr. Kernel-14B | 49.2 | 25.6 |
| Dr. Kernel-14B-STTS (last turn) | 59.8 | 31.6 |
| Dr. Kernel-14B-STTS (best-of-history) | 80.9 | 47.8 |

**STTS (Sequential Test-Time Scaling)** increases refinement turns at inference.
With context management, we keep only top-reward turns in prompt context (`w=4`) while storing full history externally.
This significantly improves Level-2 Fast@1.2.

<figure>
  <img src="{{ '/assets/img/dr-kernel/last_turn.png' | relative_url }}" alt="STTS last-turn Fast@1.2" />
  <figcaption>
    STTS, last-turn metric.
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/best_of_history.png' | relative_url }}" alt="STTS best-of-history Fast@1.2" />
  <figcaption>
    STTS, best-of-history metric.
  </figcaption>
</figure>

## A Stronger Baseline Check: `torch.compile`

We also evaluate under `torch.compile`, which is a much stronger baseline than eager mode.
Fast@p values are lower for all methods, but this setting better reflects real incremental gains.
Dr. Kernel remains competitive under this stricter evaluation.

<figure>
  <img src="{{ '/assets/img/dr-kernel/compile.png' | relative_url }}" alt="Results under torch.compile" />
  <figcaption>
    `torch.compile` evaluation: a stronger baseline where Dr. Kernel remains competitive.
  </figcaption>
</figure>

## Limitations and Next Steps

Three directions are likely the highest leverage:

- **Data scale**: expand cold-start and domain-specific kernel data.
- **Model scale**: larger models still show clear gains in this regime.
- **Productionization**: close the gap between strong generated snippets and fully autonomous end-to-end kernel optimization.

## Closing

For kernel generation, RL only works end-to-end when the system and algorithm are designed together.
The main lesson is not a single trick; it is the combination of infrastructure reliability, unbiased multi-turn learning, and bottleneck-aware objectives.

If you want to reproduce or build on this:
- Paper: [arXiv 2602.05885](https://arxiv.org/abs/2602.05885)
- Code/data/models/environment: [hkust-nlp/KernelGYM](https://github.com/hkust-nlp/KernelGYM)
- Launch thread: [X post](https://x.com/WeiLiu99/status/2019629573194019282)
