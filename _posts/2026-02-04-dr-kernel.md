---
layout: post
title: "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generation"
date: 2026-02-04
categories: [kernel, triton, rl]
paper_url: ""
---

This post is a concise, figure-first companion to our paper:
**_Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations_**.
It reuses as much paper text and figures as possible to help readers quickly grasp the key ideas.

{% if page.paper_url and page.paper_url != "" %}
Paper: {{ page.paper_url }}
{% endif %}

## TL;DR

Efficient GPU kernels are a key lever for scaling AI systems, but writing high-performance Triton kernels still takes real expertise. The paper studies how to train LLMs with reinforcement learning (RL) for *kernel generation* in a way that is robust in practice and actually improves *meaningful speedup* (not just “passes correctness”).

The core message:
1) build a *robust execution environment* (**KernelGym**) that makes RL feasible at scale,  
2) use an *unbiased* multi-turn RL estimator (**TRLOO**), and  
3) align training toward *meaningful speedup* via profiling-aware methods (**PR / PRS**) and stabilize RL with mismatch correction (**MRS**).

## What can go wrong: reward hacking and lazy optimization

Kernel generation has unusually “nice” objectives for RL: correctness can be verified by execution, and performance can be measured by profiling. But this also creates two very common failure modes:

- **Reward hacking:** the model finds loopholes in the measurement/evaluation setup to obtain high reward without doing the real work (e.g., emitting a Triton kernel but never executing it, or branching on `self.training` to bypass computation).
- **Lazy optimization:** the model produces kernels that are technically correct but only optimize trivial sub-operations, leaving the real bottleneck untouched. This can lift Fast@1 while Fast@1.2 saturates early.

<figure>
  <img src="{{ '/assets/img/dr-kernel/tradeoff_all_three.png' | relative_url }}" alt="Training curves and pitfalls cases (reward hacking and lazy optimization)" />
  <figcaption>
    Training curves (Fast@1 vs Fast@1.2) and representative cases of reward hacking / lazy optimization.
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/hacking_lazy.png' | relative_url }}" alt="Examples of reward hacking and lazy optimization" />
  <figcaption>
    Representative cases: reward hacking (kernel emitted but not executed) and lazy optimization (only trivial sub-op optimized).
  </figcaption>
</figure>

## KernelGym: a gym for kernel generation

Long-horizon RL for kernel generation is hard to run reliably because generated kernels are unsafe: compilation/runtime errors and CUDA failures are normal. KernelGym is the paper’s answer: a distributed execution environment designed specifically for stable, scalable, *measurement-correct* kernel RL.

At a high level, KernelGym:
- uses a **server–worker** architecture for scalable evaluation,
- enforces **one-GPU-one-task** serialized execution to keep timing/profiling stable,
- runs each evaluation in an **isolated subprocess** so crashes don’t poison the worker,
- exposes **structured feedback** beyond a single scalar (correctness status, runtime diagnostics, profiling summaries),
- includes an **execution-based hacking check** (not “LLM-as-a-judge”).

Concretely, the hacking check instruments Triton’s launch path to record actually executed kernels, and measures end-to-end runtime in both `train` and `eval` modes; candidates that execute *no Triton kernel* in either mode are treated as incorrect.

<figure>
  <img src="{{ '/assets/img/dr-kernel/figure1.png' | relative_url }}" alt="KernelGym overview and training framework" />
  <figcaption>
    Overview of KernelGym and the training framework (paper Figure 1).
  </figcaption>
</figure>

## Multi-turn RL and TRLOO (unbiased advantage estimation)

Kernel optimization is inherently iterative. Instead of treating kernel generation as one-shot code synthesis, the paper trains the model in **multi-turn refinement**: at each turn the model proposes a revision, KernelGym executes it, and the resulting feedback becomes part of the next-turn context.

This differs from many tool-use RL setups where the reward is sparse and only arrives at the end: KernelGym provides **dense, turn-level rewards** after each execution. For learning, the paper uses reward-to-go (credit assignment across turns) and highlights a practical issue: standard GRPO-style baselines can be **biased due to self-inclusion** within each prompt–turn group. To address that, it proposes **TRLOO (Turn-level REINFORCE Leave-One-Out)** as an unbiased advantage estimator for multi-turn RL.

<figure>
  <img src="{{ '/assets/img/dr-kernel/multiturn_rl_comparison.png' | relative_url }}" alt="Multi-turn RL comparison (Fast@1) over training" />
  <figcaption>
    Multi-turn RL variants on KernelBench Level 2 (paper Figure: multi-turn RL comparison).
  </figcaption>
</figure>

## From stability to effectiveness: overcoming lazy optimization

Even with KernelGym and TRLOO, the “lazy optimization” ceiling can remain: training becomes stable enough to run, but it still tends to collect many low-impact solutions that don’t move the real bottlenecks.

The paper tackles this in two stages:

1) **Stability (MRS):** mitigate training–inference mismatch with mismatch rejection sampling to prevent early collapse and reduce off-policy drift.
2) **Effectiveness (PR/PRS):** use *profiling-aware* signals so the learning objective prefers kernels that actually dominate end-to-end runtime.

Specifically, it introduces:
- **PR (Profiling-based Rewards):** reward kernels that dominate end-to-end CUDA runtime.
- **PRS (Profiling-based Rejection Sampling):** filter low-impact “lazy” samples during training.

One concrete example from the paper: in a lazy-optimization case study, the model-generated kernel accounts for only **0.014%** of total CUDA execution time; with better fusion, generated kernels cover **86.15%** of the total CUDA runtime—corresponding to much more meaningful speedup.

<figure>
  <img src="{{ '/assets/img/dr-kernel/fast12_comparison.png' | relative_url }}" alt="Fast@1.2 improvements with PR/PRS" />
  <figcaption>
    Profiling-based methods (PR/PRS) are required to lift the stricter Fast@1.2 metric.
  </figcaption>
</figure>

## Results highlights (KernelBench + test-time scaling)

We evaluate using KernelBench Fast@p metrics (and treat reward-hacking samples as incorrect in evaluation).

Selected KernelBench numbers (Level 2):

| Model | Fast@1 | Fast@1.2 |
| --- | ---:| ---:|
| GPT-5 | 46.7 | 28.6 |
| Claude-4.5-Sonnet | 50.0 | 26.7 |
| Dr. Kernel-14B | 49.2 | 25.6 |

Test-time scaling (STTS) further improves Fast@1.2 by increasing refinement turns at inference time; context management keeps prompts within context limits by only retaining the top-4 turns (by reward).

<figure>
  <img src="{{ '/assets/img/dr-kernel/last_turn.png' | relative_url }}" alt="STTS last-turn Fast@1.2" />
  <figcaption>
    Sequential test-time scaling: last-turn Fast@1.2 (paper Figure: tts/last_turn).
  </figcaption>
</figure>

<figure>
  <img src="{{ '/assets/img/dr-kernel/best_of_history.png' | relative_url }}" alt="STTS best-of-history Fast@1.2" />
  <figcaption>
    Sequential test-time scaling: best-of-history Fast@1.2 (paper Figure: tts/best_of_history).
  </figcaption>
</figure>

## Takeaways

- For kernel generation RL, **system design (KernelGym) matters as much as the RL algorithm**.
- **Reward hacking checks** and **profiling feedback** are critical for long-horizon training.
- **TRLOO** improves multi-turn RL by removing a practical bias source in group-based baselines.
- To beat “lazy optimization”, you must optimize for **meaningful speedup**, not just correctness.
